{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "This notebook is run on AWS EMR cluster. The notebook loads the complete log dataset(12 GB) from Sparkify from the S3 bucket.\n\nThe methods used here are taken from our work with the smaller version of this dataset which was run in a local machine. Following is the walk-through of steps done in this notebook on EMR cluster.\n\n- Load the full dataset\n- Feature engineer variables for modelling based on the analysis done with smaller dataset.\n- Vectorise and Scale the required features\n- Split training and test datasets\n- Train a Gradient Boosted Tree classifier and validate against the testing dataset. We only use GBT classifier here to validate/re-affirm our work with the smaller version of the dataset.\n- Evaluate the metrics such as Accuracy and F1 score\n- Look at important features of the model"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql import SparkSession    \nfrom pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf\nfrom pyspark.sql.types import IntegerType, DateType\nfrom pyspark.sql import Window\nimport datetime\nfrom time import time\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression, GBTClassifier, RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import StandardScaler, VectorAssembler\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2b3b4a1627b845278176577e687543c3"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1570777837805_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-5-236.ap-southeast-2.compute.internal:20888/proxy/application_1570777837805_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-11-94.ap-southeast-2.compute.internal:8042/node/containerlogs/container_1570777837805_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# create a Spark session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Sparkify\") \\\n    .getOrCreate()", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cd6def7ca8e143c3af65dbc2fbe6e8ab"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#load the complete log dataset\nevent_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\ndf = spark.read.json(event_data)\ndf.head()", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1379525e94bd498ca425cb53024c3527"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "Row(artist='Popol Vuh', auth='Logged In', firstName='Shlok', gender='M', itemInSession=278, lastName='Johnson', length=524.32934, level='paid', location='Dallas-Fort Worth-Arlington, TX', method='PUT', page='NextSong', registration=1533734541000, sessionId=22683, song='Ich mache einen Spiegel - Dream Part 4', status=200, ts=1538352001000, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='1749042')", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def feature_engineer(df):\n    #clean the dataset\n    df = df.filter(df['userId'] != '')\n    df = df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n    \n    #define churn events\n    flag_downgrade_event = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())\n    flag_churn_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n    \n    #mark downgrade and churn event on each activity\n    df = df.withColumn(\"downgrade_event\", flag_downgrade_event(\"page\"))\\\n            .withColumn(\"cancellation_event\", flag_churn_event(\"page\"))\n    \n    #label churn and downgrade event upon users\n    user_frame = Window.partitionBy('userId')\n    df = df.withColumn('downgrade', max('downgrade_event').over(user_frame)) \\\n            .withColumn('churn', max('cancellation_event').over(user_frame))\n    \n    #engineer features for modelling\n    \n    #udfs for engineering columns \n    free = udf(lambda x: int(x=='free'), IntegerType())\n    paid = udf(lambda x: int(x=='paid'), IntegerType())\n    get_day = udf(lambda x: datetime.datetime.fromtimestamp(x/1000), DateType())\n    \n    #generating featuring grouped by users\n    total_songs_listened = df \\\n        .select('userID','song') \\\n        .groupBy('userID') \\\n        .count() \\\n        .withColumnRenamed('count', 'total_songs')\n    \n    thumbs_up_count = df \\\n        .select('userID','page') \\\n        .where(df.page == 'Thumbs Up') \\\n        .groupBy('userID') \\\n        .count() \\\n        .withColumnRenamed('count', 'thumbs_up_count') \n\n    thumbs_down_count = df \\\n        .select('userID','page') \\\n        .where(df.page == 'Thumbs Down') \\\n        .groupBy('userID') \\\n        .count() \\\n        .withColumnRenamed('count', 'thumbs_down_count')\n    \n    friends_added = df.filter(df.page=='Add Friend')\\\n        .select('userId', 'page')\\\n        .groupBy('userId').count()\\\n        .withColumnRenamed('count', 'friends_added_count')\n    \n    errors_encountered = df.filter(df.page=='Error')\\\n        .select('userId', 'page', 'ts', 'length')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'})\\\n        .groupBy('userId').mean()\\\n        .withColumnRenamed('avg(count(page))', 'errors_faced')\n    \n    customer_lifetime = df \\\n        .select('userId','registration','ts') \\\n        .withColumn('lifetime',(df.ts-df.registration)) \\\n        .groupBy('userId') \\\n        .agg({'lifetime':'max'}) \\\n        .withColumnRenamed('max(lifetime)','lifetime') \\\n        .select('userId', (col('lifetime')/1000/3600/24).alias('lifetime'))\n    \n    playlist_creation = df \\\n        .select('userID','page') \\\n        .where(df.page == 'Add to Playlist') \\\n        .groupBy('userID') \\\n        .count() \\\n        .withColumnRenamed('count', 'add_to_playlist')\n    \n    help_visits = df.filter(df.page=='Help')\\\n        .select('userId', 'page', 'ts', 'length')\\\n        .withColumn('date', get_day(col('ts')))\\\n        .groupBy('userId', 'date').agg({'page':'count'})\\\n        .groupBy('userId').mean()\\\n        .withColumnRenamed('avg(count(page))', 'help_visits')\n    \n    avg_songs_per_session = df.where('page == \"NextSong\"') \\\n        .groupby(['userId', 'sessionId']) \\\n        .count() \\\n        .groupby(['userId']) \\\n        .agg({'count':'avg'}) \\\n        .withColumnRenamed('avg(count)', 'avg_songs_per_session')\n    \n    user_level = df.select('userId', 'level')\\\n        .where((df.level=='free')|(df.level=='paid'))\\\n        .dropDuplicates()\\\n        .withColumn('free_tier', free('level'))\\\n        .withColumn('paid_tier', paid('level')).drop('level')\n    \n    label = df \\\n        .select('userId', col('churn').alias('label'))\\\n        .dropDuplicates()\n    \n    #create new dataframe to be used for modelling\n    data = total_songs_listened.join(label, on='userId')\\\n        .join(user_level, on='userId')\\\n        .join(avg_songs_per_session, on='userId')\\\n        .join(help_visits, on='userId')\\\n        .join(playlist_creation, on='userId')\\\n        .join(customer_lifetime, on='userId')\\\n        .join(errors_encountered, on='userId')\\\n        .join(friends_added, on='userId')\\\n        .join(thumbs_down_count, on='userId')\\\n        .join(thumbs_up_count, on='userId')\\\n        .drop('userId')\n    \n    return data", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4e86a55f68f3469ea10f43ed184a57ef"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = feature_engineer(df)\ndf.take(2)", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "88b62d62a5574e9d8c37b7b72e9baf90"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "[Row(total_songs=1317, label=1, free_tier=1, paid_tier=0, avg_songs_per_session=48.666666666666664, help_visits=1.6, add_to_playlist=25, lifetime=77.30377314814815, errors_faced=1.0, friends_added_count=14, thumbs_down_count=33, thumbs_up_count=53), Row(total_songs=1317, label=1, free_tier=0, paid_tier=1, avg_songs_per_session=48.666666666666664, help_visits=1.6, add_to_playlist=25, lifetime=77.30377314814815, errors_faced=1.0, friends_added_count=14, thumbs_down_count=33, thumbs_up_count=53)]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def feature_scale(df):\n    features = df.drop('label').columns\n    assembler = VectorAssembler(inputCols=features, outputCol=\"NumFeatures\")\n    data=assembler.transform(df)\n    \n    scaler = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"features\", withStd=True)\n    scalerModel = scaler.fit(data)\n    data = scalerModel.transform(data)\n    \n    return data", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fc43c4c5231d4c2ab7e5f7ca703d22e5"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "data = feature_scale(df)\ndata.take(2)", "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "29275ced009a444ba647390238c5542f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "[Row(total_songs=1317, label=1, free_tier=0, paid_tier=1, avg_songs_per_session=48.666666666666664, help_visits=1.6, add_to_playlist=25, lifetime=77.30377314814815, errors_faced=1.0, friends_added_count=14, thumbs_down_count=33, thumbs_up_count=53, NumFeatures=DenseVector([1317.0, 0.0, 1.0, 48.6667, 1.6, 25.0, 77.3038, 1.0, 14.0, 33.0, 53.0]), features=DenseVector([0.8445, 0.0, 2.0021, 1.3298, 3.9489, 0.6578, 1.9802, 4.3204, 0.573, 2.2174, 0.6815])), Row(total_songs=1317, label=1, free_tier=1, paid_tier=0, avg_songs_per_session=48.666666666666664, help_visits=1.6, add_to_playlist=25, lifetime=77.30377314814815, errors_faced=1.0, friends_added_count=14, thumbs_down_count=33, thumbs_up_count=53, NumFeatures=DenseVector([1317.0, 1.0, 0.0, 48.6667, 1.6, 25.0, 77.3038, 1.0, 14.0, 33.0, 53.0]), features=DenseVector([0.8445, 2.0021, 0.0, 1.3298, 3.9489, 0.6578, 1.9802, 4.3204, 0.573, 2.2174, 0.6815]))]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train, rest = data.randomSplit([0.6, 0.4], seed=42)\nvalidation, test = rest.randomSplit([0.5,0.5], seed=42)", "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dc7a5e5234854172b77131b18bf2f83c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "gbt = GBTClassifier(maxIter=10,seed=42)\ngbt_model = gbt.fit(train)\nresults_final = gbt_model.transform(test)", "execution_count": 10, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "10b540c5d7cc452f83b29718c0ea73f3"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nprint('Metrics:')\nprint('Accuracy: {}'.format(evaluator.evaluate(results_final, {evaluator.metricName: \"accuracy\"})))\nprint('F-1 Score:{}'.format(evaluator.evaluate(results_final, {evaluator.metricName: \"f1\"})))", "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "80723a69652d4f179a96b170adc7f4e0"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "Metrics:\nAccuracy: 0.8326756116811366\nF-1 Score:0.8054970355804507", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "feature_ind = gbt_model.featureImportances.indices.tolist()\nfeatures = df.drop('label').columns\nfeature_name = [features[ind] for ind in feature_ind]\nfeature_coef = gbt_model.featureImportances.values.tolist()\nprint(feature_name)\nprint()\nprint(feature_coef)", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9c92c733f4bc4c468b8ffadd25e2c802"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "['total_songs', 'avg_songs_per_session', 'help_visits', 'add_to_playlist', 'lifetime', 'errors_faced', 'friends_added_count', 'thumbs_down_count', 'thumbs_up_count']\n\n[0.1102245511111617, 0.12269378173912349, 0.06958595730175013, 0.08590240367675563, 0.23165153605701336, 0.021539120434971665, 0.07819213119609128, 0.137730723894722, 0.14247979458841073]", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Accuracy and F1 score of the model has dropped when trained on a huge dataset. But the accuracy is still around 80% which is quite good for finding if a user will cancel subscription or not. \n\nThe only difference is the importance of features has changed for a larger dataset. Here, it seems like Customer lifetime is very important. Loyal customers do not churn easily. Other than that, features like total songs played, songs listened per session and thumbs up event does take part in predicting a customer churn.\n\nUsing this model, a company could predict the churn percentage and then touch base with them to identify their problems with the help of segmented marketing."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}